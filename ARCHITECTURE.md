# MQTT v5.0 Client Library Architecture

## CRITICAL ARCHITECTURAL PRINCIPLE: NO EVENT LOOPS

**THIS IS A RUST ASYNC LIBRARY. WE DO NOT USE EVENT LOOPS.**

Rust's async ecosystem is fundamentally different from other languages. It's based on:
- State machines generated by the compiler
- Futures that represent asynchronous computations
- The async runtime (Tokio) that polls these futures

We do NOT need or want event loops because:
1. Tokio already provides the async runtime
2. Event loops are redundant and add unnecessary complexity
3. Direct async/await is more efficient and idiomatic in Rust

### FORBIDDEN Patterns

The following patterns are STRICTLY FORBIDDEN in this codebase:

```rust
// FORBIDDEN: Event loop pattern
loop {
    tokio::select! {
        Some(cmd) = command_rx.recv() => { /* handle command */ }
        Some(packet) = packet_rx.recv() => { /* handle packet */ }
        _ = interval.tick() => { /* periodic task */ }
    }
}
```

```rust
// FORBIDDEN: Command channel pattern
enum Command {
    Connect { addr: String, response: oneshot::Sender<Result<()>> },
    Publish { topic: String, payload: Vec<u8>, response: oneshot::Sender<Result<()>> },
}
// DO NOT create command channels and event loops to process them
```

```rust
// FORBIDDEN: Polling loops
loop {
    if let Some(packet) = check_for_packet() {
        handle_packet(packet);
    }
    tokio::time::sleep(Duration::from_millis(10)).await;
}
```

### REQUIRED Patterns

Use these patterns instead:

```rust
// CORRECT: Direct async methods
impl MqttClient {
    pub async fn connect(&self, address: &str) -> Result<()> {
        // Direct connection logic
        let transport = TcpTransport::connect(address).await?;
        transport.write_packet(connect_packet).await?;
        let connack = transport.read_packet().await?;
        Ok(())
    }
    
    pub async fn publish(&self, topic: &str, payload: &[u8]) -> Result<()> {
        // Direct publish - no indirection
        let transport = self.transport.read().await;
        transport.write_packet(publish_packet).await?;
        Ok(())
    }
}
```

```rust
// CORRECT: Background tasks for continuous operations
async fn packet_reader_task(client: Arc<MqttClient>) {
    let transport = client.transport.read().await;
    // Direct reading - the await will suspend until data arrives
    while let Ok(packet) = transport.read_packet().await {
        client.handle_packet(packet).await;
    }
}

// Spawn it once during connection
tokio::spawn(packet_reader_task(client.clone()));
```

```rust
// CORRECT: Timers and periodic tasks
async fn keepalive_task(client: Arc<MqttClient>) {
    let mut interval = tokio::time::interval(Duration::from_secs(30));
    loop {
        interval.tick().await;
        if let Err(e) = client.send_ping().await {
            break;
        }
    }
}
```

## Architecture Overview

### Core Components

1. **MqttClient**: The main client struct
   - Holds shared state (transport, session, callbacks)
   - Provides direct async methods for all operations
   - NO event loop, NO command channels

2. **Transport Layer**: Direct async I/O
   - `read_packet()` - async method that waits for next packet
   - `write_packet()` - async method that sends a packet
   - Implemented for TCP, TLS, WebSocket

3. **Session State**: Shared state management
   - Wrapped in Arc<RwLock<T>> for concurrent access
   - Accessed directly by client methods
   - NO message passing, NO commands

4. **Background Tasks**: Specific focused tasks
   - Packet reader: Continuously reads packets
   - Keep-alive: Sends pings at intervals
   - Reconnection: Handles connection recovery
   - Each is a simple async function, NOT an event loop

### Data Flow

1. **Incoming Packets**:
   ```
   Network -> Transport.read_packet() -> packet_reader_task -> handle_packet() -> callbacks
   ```

2. **Outgoing Operations**:
   ```
   Client method -> Transport.write_packet() -> Network
   ```

3. **No Indirection**: Operations go directly from API call to network I/O

## Implementation Guidelines

1. **Every client method should be async and direct**
   - No sending commands to event loops
   - No waiting for responses through channels
   - Direct transport operations

2. **Background tasks should be simple and focused**
   - One task per concern (reading, keep-alive, etc.)
   - Use async/await, not select! loops
   - Let Tokio handle the scheduling

3. **State management through shared ownership**
   - Use Arc<RwLock<T>> or Arc<Mutex<T>>
   - Access state directly in methods
   - No actor pattern, no message passing

4. **Error handling is direct**
   - Errors propagate through Result<T, E>
   - No error channels or error events
   - Handle errors at the call site

## Why This Matters

1. **Performance**: Direct calls are faster than message passing
2. **Simplicity**: Less code, easier to understand
3. **Debugging**: Direct call stacks, no event loop indirection
4. **Idiomatic**: This is how Rust async is meant to be used
5. **Maintainability**: Clear data flow, obvious dependencies

## For Future Contributors

If you're coming from Node.js, Python asyncio, or other event-loop-based systems:
- Forget everything you know about event loops
- Rust async is different and better
- Trust the async/await pattern
- Let Tokio do the hard work

**Remember: In Rust async, we write straight-line async code and let the compiler and runtime handle the complexity.**