# MQTT v5.0 Platform Architecture

This document describes the architecture of both the MQTT client library and broker implementation.

## Core Architectural Principle: Direct Async/Await

This library uses Rust's native async/await patterns throughout. Rust's async ecosystem is based on:
- State machines generated by the compiler
- Futures that represent asynchronous computations
- The async runtime (Tokio) that polls these futures

We use direct async/await patterns because:
1. Tokio already provides the async runtime
2. Direct async calls are more efficient and idiomatic in Rust
3. The code is simpler and easier to debug

### Patterns to Avoid

The following patterns should be avoided in favor of direct async/await:

```rust
// Avoid: Event loop pattern
loop {
    tokio::select! {
        Some(cmd) = command_rx.recv() => { /* handle command */ }
        Some(packet) = packet_rx.recv() => { /* handle packet */ }
        _ = interval.tick() => { /* periodic task */ }
    }
}
```

```rust
// Avoid: Command channel pattern
enum Command {
    Connect { addr: String, response: oneshot::Sender<Result<()>> },
    Publish { topic: String, payload: Vec<u8>, response: oneshot::Sender<Result<()>> },
}
// Prefer direct async methods instead
```

```rust
// Avoid: Polling loops
loop {
    if let Some(packet) = check_for_packet() {
        handle_packet(packet);
    }
    tokio::time::sleep(Duration::from_millis(10)).await;
}
```

### REQUIRED Patterns

Use these patterns instead:

```rust
// CORRECT: Direct async methods
impl MqttClient {
    pub async fn connect(&self, address: &str) -> Result<()> {
        // Direct connection logic
        let transport = TcpTransport::connect(address).await?;
        transport.write_packet(connect_packet).await?;
        let connack = transport.read_packet().await?;
        Ok(())
    }
    
    pub async fn publish(&self, topic: &str, payload: &[u8]) -> Result<()> {
        // Direct publish - no indirection
        let transport = self.transport.read().await;
        transport.write_packet(publish_packet).await?;
        Ok(())
    }
}
```

```rust
// CORRECT: Background tasks for continuous operations
async fn packet_reader_task(client: Arc<MqttClient>) {
    let transport = client.transport.read().await;
    // Direct reading - the await will suspend until data arrives
    while let Ok(packet) = transport.read_packet().await {
        client.handle_packet(packet).await;
    }
}

// Spawn it once during connection
tokio::spawn(packet_reader_task(client.clone()));
```

```rust
// CORRECT: Timers and periodic tasks
async fn keepalive_task(client: Arc<MqttClient>) {
    let mut interval = tokio::time::interval(Duration::from_secs(30));
    loop {
        interval.tick().await;
        if let Err(e) = client.send_ping().await {
            break;
        }
    }
}
```

## Architecture Overview

### Core Components

1. **MqttClient**: The main client struct
   - Holds shared state (transport, session, callbacks)
   - Provides direct async methods for all operations
   - Uses direct async/await patterns

2. **Transport Layer**: Direct async I/O
   - `read_packet()` - async method that waits for next packet
   - `write_packet()` - async method that sends a packet
   - Implemented for TCP, TLS, WebSocket

3. **Session State**: Shared state management
   - Wrapped in Arc<RwLock<T>> for concurrent access
   - Accessed directly by client methods
   - Direct state access pattern

4. **Background Tasks**: Specific focused tasks
   - Packet reader: Continuously reads packets
   - Keep-alive: Sends pings at intervals
   - Reconnection: Handles connection recovery
   - Each is a simple async function

### Data Flow

1. **Incoming Packets**:
   ```
   Network -> Transport.read_packet() -> packet_reader_task -> handle_packet() -> callbacks
   ```

2. **Outgoing Operations**:
   ```
   Client method -> Transport.write_packet() -> Network
   ```

3. **No Indirection**: Operations go directly from API call to network I/O

## Implementation Guidelines

1. **Every client method should be async and direct**
   - Direct transport operations
   - Immediate execution without indirection
   - Clear async/await flow

2. **Background tasks should be simple and focused**
   - One task per concern (reading, keep-alive, etc.)
   - Use async/await, not select! loops
   - Let Tokio handle the scheduling

3. **State management through shared ownership**
   - Use Arc<RwLock<T>> or Arc<Mutex<T>>
   - Access state directly in methods
   - No actor pattern, no message passing

4. **Error handling is direct**
   - Errors propagate through Result<T, E>
   - No error channels or error events
   - Handle errors at the call site

## Why This Matters

1. **Performance**: Direct calls are faster than message passing
2. **Simplicity**: Less code, easier to understand
3. **Debugging**: Direct call stacks with clear async flow
4. **Idiomatic**: This is how Rust async is meant to be used
5. **Maintainability**: Clear data flow, obvious dependencies

## For Future Contributors

If you're coming from other async systems:
- Rust uses native async/await patterns
- The Tokio runtime handles task scheduling
- Write straight-line async code
- Let the compiler and runtime handle the complexity

## Broker Architecture

The MQTT broker follows the same architectural principles as the client - direct async/await patterns.

### Broker Core Components

1. **MqttBroker**: The main broker struct
   - Manages configuration and lifecycle
   - Spawns listening tasks for each transport
   - Direct async operations

2. **Server Listeners**: One per transport type
   - TCP listener: Direct `accept()` loop
   - TLS listener: TLS wrapper around TCP
   - WebSocket listener: HTTP upgrade handling
   - Each spawns client handlers directly

3. **ClientHandler**: Per-client connection handler
   - Direct async packet reading and writing
   - Manages client session state
   - Handles MQTT protocol directly
   - Simple async task pattern

4. **Message Router**: Subscription matching and delivery
   - Optimized trie-based topic matching
   - Direct message routing to subscribers
   - Shared subscription support
   - Thread-safe concurrent access

5. **Storage Backend**: Persistence layer
   - Sessions, retained messages, queued messages
   - File-based or in-memory implementations
   - Direct async I/O operations

### Broker Data Flow

1. **Connection Acceptance**:
   ```
   TCP/TLS/WS Listener -> accept() -> spawn(ClientHandler::new())
   ```

2. **Packet Processing**:
   ```
   Client -> Transport.read_packet() -> ClientHandler.handle_packet() -> Router/Storage
   ```

3. **Message Routing**:
   ```
   Publisher -> Router.route_message() -> matching subscribers -> Transport.write_packet()
   ```

### Key Broker Patterns

```rust
// CORRECT: Direct connection handling
impl Server {
    async fn run(&mut self) -> Result<()> {
        loop {
            let (stream, addr) = self.listener.accept().await?;
            let handler = ClientHandler::new(stream, addr, self.router.clone());
            tokio::spawn(handler.run());
            // Direct spawn of handler task
        }
    }
}
```

```rust
// CORRECT: Direct packet handling in ClientHandler
impl ClientHandler {
    async fn run(mut self) -> Result<()> {
        while let Ok(packet) = self.transport.read_packet().await {
            match packet {
                Packet::Publish(pub_packet) => {
                    self.handle_publish(pub_packet).await?;
                }
                Packet::Subscribe(sub_packet) => {
                    self.handle_subscribe(sub_packet).await?;
                }
                // ... direct handling for each packet type
            }
        }
        Ok(())
    }
}
```

```rust
// CORRECT: Direct message routing
impl MessageRouter {
    pub async fn route_message(&self, topic: &str, message: PublishPacket) {
        let subscribers = self.find_matching_subscribers(topic).await;
        for subscriber in subscribers {
            // Direct delivery to subscriber
            subscriber.deliver_message(message.clone()).await;
        }
    }
}
```

### Broker-Specific Components

1. **Authentication Manager**:
   - Direct auth checks during CONNECT
   - Pluggable auth providers (password, certificate, etc.)
   - Integrated auth checking

2. **ACL Manager**:
   - Direct authorization checks for publish/subscribe
   - Rule-based access control
   - Integrated with packet handlers

3. **Resource Monitor**:
   - Tracks connections, bandwidth, messages
   - Enforces rate limits and quotas
   - Direct checks, no monitoring loops

4. **Bridge Manager**:
   - Manages broker-to-broker connections
   - Each bridge is a client to remote broker
   - Direct message forwarding based on rules

5. **$SYS Topics Provider**:
   - Publishes broker statistics
   - Simple periodic task
   - Direct publish to router

### Performance Optimizations

1. **Connection Pooling**:
   - Reuse buffers and resources
   - Pre-allocated packet buffers
   - Efficient memory management

2. **Optimized Router**:
   - Trie-based topic matching
   - O(log n) subscription lookups
   - Lock-free read operations

3. **Zero-Copy Operations**:
   - Direct buffer passing where possible
   - Avoid unnecessary allocations
   - Efficient packet serialization

### Scalability Patterns

1. **Horizontal Scaling**:
   - Multiple broker instances
   - Shared-nothing architecture
   - Bridge connections for clustering

2. **Vertical Scaling**:
   - Multi-core utilization via Tokio
   - Work-stealing task scheduler
   - Concurrent client handling

3. **Resource Limits**:
   - Per-client and global limits
   - Backpressure mechanisms
   - Graceful degradation

### Broker Implementation Guidelines

1. **Every operation should be direct async**:
   - Direct async operations
   - No central message bus
   - Direct handling at each layer

2. **Per-client isolation**:
   - Each ClientHandler is independent
   - Failure isolation
   - No shared mutable state beyond router

3. **Lock-free where possible**:
   - RwLock for read-heavy operations
   - Arc for shared immutable data
   - Atomic operations for counters

4. **Monitoring without loops**:
   - Metrics updated inline during operations
   - Periodic tasks for aggregation only
   - No separate monitoring loops

## Platform Integration

Both client and broker share:

1. **Common Protocol Implementation**:
   - Shared packet encoding/decoding
   - Same MQTT v5.0 compliance
   - Reusable validation logic

2. **Transport Abstraction**:
   - Same TCP/TLS/WebSocket code
   - Unified connection handling
   - Shared TLS configuration

3. **Architectural Principles**:
   - Direct async/await throughout
   - Direct async/await throughout
   - Shared error handling patterns

## Testing Architecture

1. **Unit Tests**:
   - Direct testing of components
   - Simple async patterns
   - Fast and deterministic

2. **Integration Tests**:
   - Full client-broker communication
   - Network simulation with Turmoil
   - Property-based testing

3. **Performance Tests**:
   - Direct benchmarking
   - Direct execution paths
   - Realistic load testing

## Future Considerations

When adding new features:

1. **Maintain direct async/await patterns**
2. **Keep operations direct and async**
3. **Avoid indirection and message passing**
4. **Trust Rust's async/await model**
5. **Let Tokio handle concurrency**

The architecture is designed for:
- Maximum performance
- Code clarity
- Maintainability
- Idiomatic Rust patterns

**Both client and broker demonstrate that complex networked systems can be built with direct async/await patterns, resulting in cleaner, faster, and more maintainable code.**